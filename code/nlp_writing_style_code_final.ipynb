{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_writing_style_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9K-0anc1KxG",
        "outputId": "9b66c316-d603-44fe-b2c3-586fff1f08a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puFdgUFi1WiG"
      },
      "source": [
        "# 라이브러리\n",
        "import pandas as pd\n",
        "import warnings \n",
        "warnings.filterwarnings(action='ignore')\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# TF-IDF Vectorization 라이브러리\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63j-em4a1q53"
      },
      "source": [
        "# 경로 설정\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0MNiOb61ua2"
      },
      "source": [
        "# 파일 불러오기\n",
        "train = pd.read_csv('open/train.csv', encoding = 'utf-8')\n",
        "test = pd.read_csv('open/test_x.csv', encoding = 'utf-8')\n",
        "sample_submission = pd.read_csv('open/sample_submission.csv', encoding = 'utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqUuwA_41vab"
      },
      "source": [
        "# 텍스트 전처리 함수 정의\n",
        "# 한글, 알파벳, 숫자, 그리고 문장부호 중 ;와 :을 제외하고는 공백 처리하는 함수\n",
        "def alpha_num(string):\n",
        "    # 정규표현식을 이용한 전처리\n",
        "    string = re.sub(r\"[^가-힣A-Za-z0-9;:!?]\", \" \", string) # 한글, 알파벳, 숫자, ;: 제외하고는 모두 공백 처리\n",
        "    string = re.sub(r\"\\'s\", \" 's\", string)                # 's 앞에 공백 추가\n",
        "    string = re.sub(r\"\\'ve\", \" 've\", string)              # 've 앞에 공백 추가\n",
        "    string = re.sub(r\"\\'t\", \" 't\", string)                # 't 앞에 공백 추가\n",
        "    string = re.sub(r\"\\'re\", \" 're\", string)              # 're 앞에 공백 추가\n",
        "    string = re.sub(r\"\\'d\", \" 'd\", string)                # 'd 앞에 공백 추가\n",
        "    string = re.sub(r\"\\'ll\", \" 'll\", string)              # 'll 앞에 공백 추가\n",
        "    string = re.sub(r\",\", \" \", string)                    # 쉼표 제거\n",
        "    string = re.sub(r\"!\", \" ! \", string)                  # 느낌표 ! 앞뒤로 공백 추가\n",
        "    string = re.sub(r\"\\(\", \" ( \", string)                 # 여는 괄호 ( 앞뒤로 공백 추가\n",
        "    string = re.sub(r\"\\)\", \" ) \", string)                 # 닫는 괄호 ) 앞뒤로 공백 추가\n",
        "    string = re.sub(r\"\\?\", \" ? \", string)                 # 물음표 ? 앞뒤로 공백 추가\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)               # 2개 이상의 공백을 공백 1개로 변경\n",
        "    string = re.sub(r\"\\'{2,}\", \" \", string)               # 2개 이상의 작은 따옴표 제거\n",
        "    string = re.sub(r\"\\'\", \" \", string)                   # 작은 따옴표 ' 제거\n",
        "    string = re.sub(r\";\", \" ; \", string)                  # 세미콜론 ; 앞뒤로 공백 추가\n",
        "    string = re.sub(r\"\\\"\", \"\", string)                    # 큰 따옴표 \" 제거\n",
        "    string = re.sub(r\"\\.\", \"  \", string)                  # 온점 제거\n",
        "\n",
        "    return string\n",
        "\n",
        "\n",
        "# Train 데이터 전처리\n",
        "train['text']=train['text'].apply(alpha_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stUtMnBi1zUd"
      },
      "source": [
        "# 불용어 제거 함수 정의\n",
        "def remove_stopwords(text):\n",
        "    # 최종 텍스트들을 저장할 배열 생성\n",
        "    final_text = []\n",
        "    # # text을 split()하여 i로 전달\n",
        "    for i in text.split():\n",
        "        # 데이콘 제공 불용어 데이터와 겹치지 않는 데이터의 공백을 없애고 소문자로 변환\n",
        "        if i.strip().lower() not in stopwords:\n",
        "            # final_text 배열에 공백 제거 후 저장\n",
        "            final_text.append(i.strip())\n",
        "\n",
        "    # \" \"문자열의 문자와 문자 사이에 삽입하는 join 메소드\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "\n",
        "# DACON 제공 기준 불용어 목록 -> 최종적으로 사용하지 않음\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
        "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
        "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
        "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
        "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
        "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
        "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
        "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
        "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
        "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
        "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
        "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
        "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
        "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
        "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
        "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
        "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
        "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
        "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
        "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
        "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
        "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
        "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
        "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
        "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
        "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
        "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
        "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
        "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
        "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
        "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
        "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
        "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
        "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
        "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
        "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
        "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
        "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
        "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
        "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
        "    \"yourselves\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf4h86KI13MA"
      },
      "source": [
        "# 텍스트 데이터 전처리\n",
        "\n",
        "# 소문자로 변경\n",
        "train['text'] = train['text'].str.lower()\n",
        "test['text'] = test['text'].str.lower()\n",
        "\n",
        "# 불용어 함수 적용\n",
        "train['text'] = train['text'].apply(alpha_num).apply(remove_stopwords)\n",
        "test['text'] = test['text'].apply(alpha_num).apply(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CP_lXG3gA2N"
      },
      "source": [
        "## 텍스트 데이터 시각화를 위한 전처리 과정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tou0Xfq8gFLb"
      },
      "source": [
        "# 작가별 text 나누는 함수 정의\n",
        "author_0 = train.loc[train['author'] == 0, 'text']\n",
        "author_1 = train.loc[train['author'] == 1, 'text']\n",
        "author_2 = train.loc[train['author'] == 2, 'text']\n",
        "author_3 = train.loc[train['author'] == 3, 'text']\n",
        "author_4 = train.loc[train['author'] == 4, 'text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOxn4ohsgHeG"
      },
      "source": [
        "# 시리즈를 문자열 데이터로 바꾸는 함수 정의\n",
        "def series_list_to_str(a):\n",
        "  # 시리즈를 리스트로 변환\n",
        "  a.to_list()\n",
        "  # 리스트를 문자열로 변환\n",
        "  return \" \".join([str(_) for _ in a])\n",
        "\n",
        "# 작가 별 단어 사용 시각화를 위해 따로 저장\n",
        "str_author_0 = series_list_to_str(author_0)\n",
        "str_author_1 = series_list_to_str(author_1)\n",
        "str_author_2 = series_list_to_str(author_2)\n",
        "str_author_3 = series_list_to_str(author_3)\n",
        "str_author_4 = series_list_to_str(author_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoXuel75fzQ3"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "# 단어 빈도 수에 따라 랭킹을 부여하는 함수 정의\n",
        "def fitontexts(words):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts([words])\n",
        "  return tokenizer.word_index\n",
        "\n",
        "# 각 작가 별로 빈도 수가 높은 단어 순으로 저장\n",
        "author0_index = fitontexts(author_word_0)\n",
        "author1_index = fitontexts(author_word_1)\n",
        "author2_index = fitontexts(author_word_2)\n",
        "author3_index = fitontexts(author_word_3)\n",
        "author4_index = fitontexts(author_word_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgfGkj2GfzZo"
      },
      "source": [
        "# 시각화를 위한 라이브러리 \n",
        "import seaborn as sns\n",
        "from collections import  Counter\n",
        "# figure 사이즈 지정\n",
        "rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# 각 작가 별 상위 30개 단어 시각화\n",
        "def plot_top_non_stopwords_barchart(text):    \n",
        "    new = text.str.split()\n",
        "    new = new.values.tolist()\n",
        "    corpus = [word for i in new for word in i]\n",
        "    counter = Counter(corpus)\n",
        "    most = counter.most_common()\n",
        "    x, y=[], []\n",
        "    # 상위 30개 단어 시각화\n",
        "    for word,count in most[:30]:\n",
        "          x.append(word)\n",
        "          y.append(count)\n",
        "            \n",
        "    sns.barplot(x=y, y=x)\n",
        "\n",
        "# Author 4의 탑 30개 단어 시각화 - 같은 방식으로 나머지 작가들도 시각화  \n",
        "plot_top_non_stopwords_barchart(author_4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qls30BWFfzbG"
      },
      "source": [
        "# 모든 작가의 상위 10개 단어 시각화\n",
        "fig = plt.figure(figsize=(30,20))\n",
        "\n",
        "def plot_top_non_stopwords_barchart2(text):\n",
        "    new= text.str.split()\n",
        "    new=new.values.tolist()\n",
        "    corpus=[word for i in new for word in i]\n",
        "    counter=Counter(corpus)\n",
        "    most=counter.most_common()\n",
        "    x, y=[], []\n",
        "    # 상위 10개 단어 시각화\n",
        "    for word,count in most[:10]:\n",
        "          x.append(count)\n",
        "          y.append(word)\n",
        "    return x, y\n",
        "\n",
        "for i in range(5):\n",
        "    x,y = plot_top_non_stopwords_barchart2(train[train['author']==i]['text'])\n",
        "    ax = fig.add_subplot(3, 2, i+1)\n",
        "    ax.set_title(i)\n",
        "    sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdLvvswYiVJC"
      },
      "source": [
        "## 시각화 결과\n",
        "* 실험 1: 불용어를 포함하여 작가 별 단어 사용 시각화\n",
        "* 실험 2: 불용어를 제외한 작가 별 단어 사용 시각화\n",
        "* 실험 3: 불용어를 제외한 후 특수 문장부호 (;, :, ?, !) 포함 시각화\n",
        "* 실험 4(최종): 불용어를 제외한 후 콜론과 세미콜론만 포함 시각화 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx7KF6MBi6OC"
      },
      "source": [
        "## 소설 작가 분류를 위한 코드 작성\n",
        "* 불용어의 경우 기존 DACON 코드보다 NLTK 라이브러리가 더 자세하고 효과적이라는 결론으로 NLTK 불용어 샘플을 활용하여 모델 작성\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAVuv8GRjdsS"
      },
      "source": [
        "# 파일 다시 불러오기\n",
        "train = pd.read_csv('open/train.csv', encoding = 'utf-8')\n",
        "test = pd.read_csv('open/test_x.csv', encoding = 'utf-8')\n",
        "sample_submission = pd.read_csv('open/sample_submission.csv', encoding = 'utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3742Tz3jaT6"
      },
      "source": [
        "# NLTK 라이브러리에서 제공하는 불용어 리스트 사용\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# nltk 라이브러리에서 제공하는 영어 불용어 샘플을 변수에 저장\n",
        "stw = set(stopwords.words('english'))\n",
        "\n",
        "# 불용어 제거 함수 정의\n",
        "def remove_stopwords(text):\n",
        "    # 최종 텍스트들을 저장할 배열 생성\n",
        "    final_text = []\n",
        "    # text을 split()하여 i로 전달\n",
        "    for i in text.split():\n",
        "        # 불용어 데이터와 겹치지 않는 데이터의 공백을 없애고 소문자로 변환\n",
        "        if i.strip().lower() not in stw:\n",
        "            # final_text 배열에 공백 제거 후 저장\n",
        "            final_text.append(i.strip())\n",
        "    \n",
        "    # \" \"문자열의 문자와 문자 사이에 삽입하는 join 메소드\n",
        "    return \" \".join(final_text)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-UY-0glkEdc"
      },
      "source": [
        "# 전처리 함수의 경우 기존의 alpha_num을 그대로 사용\n",
        "\n",
        "#소문자로 변경\n",
        "train['text'] = train['text'].str.lower()\n",
        "test['text'] = test['text'].str.lower()\n",
        "\n",
        "# 기존의 alpha_num과 불용어 함수 적용\n",
        "train['text'] = train['text'].apply(alpha_num).apply(remove_stopwords)\n",
        "test['text'] = test['text'].apply(alpha_num).apply(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIWLYyeG0lq9"
      },
      "source": [
        "# 토크나이징을 위해 필요한 punkt 설치\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 워드 토큰화 + 스팀, 레만 \n",
        "# punk_sent_tokenize = PunktSentenceTokenizer()\n",
        "# sent_tokenize = sent_tokenize()\n",
        "def preprocess_text(text):\n",
        "        corpus=[]\n",
        "        stem=PorterStemmer()\n",
        "        lem=WordNetLemmatizer()\n",
        "        for news in text:\n",
        "            # 필요할 시 word_tokenize가 아닌 다른 라이브러리 사용 가능\n",
        "            words=[w for w in word_tokenize(news) \n",
        "                   # 불용어 샘플 활용시 주석 해제 후 적용\n",
        "                   # stopw = set(stopwords.words('english'))\n",
        "                   # if (w not in stopw)\n",
        "                  ]\n",
        "            # stemming: PorterStemmer 사용 - stem\n",
        "            # lemmatization: WordNetLemaatizer 사용 - lem\n",
        "            # words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
        "            corpus.append(words)\n",
        "        return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPDoF8_Xfzec"
      },
      "source": [
        "# corpus에 최종 전처리된 데이터를 넣음 + 예시 출력\n",
        "corpus = preprocess_text(train['text'])\n",
        "# corpus[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej02YMkNfzgi"
      },
      "source": [
        "# train test 분리\n",
        "X_train = np.array([x for x in train['text']])\n",
        "X_test = np.array([x for x in test['text']])\n",
        "y_train = np.array([x for x in train['author']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPuLaCEcfzk1"
      },
      "source": [
        "# TF-IDF를 통한 Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# TF-IDF를 통한 Vectorization 결과\n",
        "# print(vectorizer.get_feature_names())\n",
        "# print(vectors.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-p-2pmdfznj"
      },
      "source": [
        "# 파라미터 값 설정\n",
        "vocab_size = 53968\n",
        "embedding_dim = 16\n",
        "max_length = 212\n",
        "padding_type='post'\n",
        "#oov_tok = \"<OOV>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWWW5b1Mfzpv"
      },
      "source": [
        "# tokenizer에 fit\n",
        "tokenizer = Tokenizer(num_words = vocab_size)#, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asrATzHXfzr5"
      },
      "source": [
        "# 데이터 sequence 변환 및 패딩 적용\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "test_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5VKUBiQmnwu"
      },
      "source": [
        "## 임베딩을 위한 코드\n",
        "### Word2Vec Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oViES6sMfzt7"
      },
      "source": [
        "# Word2Vec 임베딩을 하기 위한 코드\n",
        "\n",
        "import gensim\n",
        "\n",
        "# word2vec이 아직 없다면 주석을 해제하고 설치\n",
        "# !wget \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" \n",
        "\n",
        "# word2vec 모델 로드\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLZCq9Gb2BWQ"
      },
      "source": [
        "# Word2Vec의 경우 300 차원의 임베딩 매트릭스 생성\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "\n",
        "# 토큰 별로 벡터를 구하는 함수\n",
        "def get_vector(word):\n",
        "    if word in word2vec_model:\n",
        "        return word2vec_model[word]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# 토큰들의 벡터를 임베딩 메트릭스에 저장\n",
        "for word, i in word_index.items(): \n",
        "    temp = get_vector(word) \n",
        "    if temp is not None: \n",
        "        embedding_matrix[i] = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J1DSvGkm_1v"
      },
      "source": [
        "### Glove Embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQwq_YStm5Sp"
      },
      "source": [
        "# 임베딩 글로브\n",
        "vocab = nltk.FreqDist(np.hstack(train['text']))\n",
        "\n",
        "# Glove가 아직 없다면 주석을 해제하고 설치\n",
        "# !wget \"https://drive.google.com/file/d/1yHGtccC2FV3_d6C6_Q4cozYSOgA7bG-e/view\" \n",
        "\n",
        "glove = dict()\n",
        "\n",
        "# Word Embeddings 폴더 안의 glove.txt \n",
        "f = open('word-embeddings/glove/glove.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:], dtype='float32')\n",
        "    glove[word] = vector\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fjXzdu3m5dH"
      },
      "source": [
        "# Glove의 경우 100차원의 임베딩 매트릭스 생성\n",
        "embedding_matrix = np.zeros((len(vocab), 100)) \n",
        "\n",
        "for index, word in enumerate(vocab.items()):\n",
        "    # 토큰이 이미 사전학습된 glove의 목록에 존재하는 경우\n",
        "    if word[0] in glove:\n",
        "        # 해당 토큰의 벡터를 이용해 임베딩 메트릭스에 저장\n",
        "        embedding_vector = glove[word[0]]\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMQkqS1nn5Fu"
      },
      "source": [
        "## 자연어 처리 모델 구현\n",
        "\n",
        "### CNN 모델\n",
        "* CNN 모델은 Word2Vec 임베딩 기준으로 올린다. \n",
        "* Glove 임베딩 역시 동일하게 임베딩한 후 임베딩 계층에 그대로 넣으면 된다\n",
        "* 이하의 다른 모델들 역시 Word2Vec 임베딩 조합으로 작성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5snxaILX8_Mw"
      },
      "source": [
        "# Word2Vec 임베딩 + CNN 모델\n",
        "# Glove 역시 word2vec과 동일한 방식으로 구현\n",
        "\n",
        "# 모델링을 위한 텐서플로 케라스 라이브러리\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam, RMSProp\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# 드롭아웃 값 정의\n",
        "DROP_OUT = 0.5\n",
        "\n",
        "# Tensorflow Keras NLP 모델 생성 - 파이프라인 제공 코드에서 Embedding 레이어만 변경\n",
        "model = Sequential([\n",
        "    # Word2Vec 기반 Embedding 레이어 - 위에서 정의한 word2vec 임베딩 메트릭스 활용\n",
        "    # 마찬가지로 glove 방식 또한 사전 정의한 임베딩 메트릭스 활용\n",
        "    Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = max_length),\n",
        "    # Dropout 레이어\n",
        "    Dropout(DROP_OUT),\n",
        "    # 컨볼루션 레이어\n",
        "    Conv1D(filters = 128, kernel_size = 7, padding=\"valid\", activation=\"relu\", strides=3),   \n",
        "    Conv1D(filters = 128, kernel_size = 7, padding=\"valid\", activation=\"relu\", strides=3),  \n",
        "    Conv1D(filters = 128, kernel_size = 7, padding=\"valid\", activation=\"relu\", strides=3),   \n",
        "    # MaxPooling 레이어\n",
        "    GlobalMaxPooling1D(),\n",
        "    # Dense 레이어 (relu 활성화 함수)\n",
        "    Dense(128, activation='relu'),\n",
        "    # Dropout 레이어\n",
        "    Dropout(DROP_OUT),\n",
        "    # Dense 레이어 (softmax 활성화 함수)\n",
        "    Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3YJg0I07_tu"
      },
      "source": [
        "# 모델 학습\n",
        "num_epochs = 20\n",
        "history = model.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2, \n",
        "                    validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOBncpcx7KYR"
      },
      "source": [
        "# fit model\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
        "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "num_epochs = 20\n",
        "\n",
        "history = model.fit(train_padded, y_train, epochs=num_epochs, verbose=2, validation_split=0.2, callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ON5uJJtkMEG"
      },
      "source": [
        "### LSTM 모델\n",
        "* CNN과 동일하게 텐서플로 케라스를 활용해 LSTM 모델 구축했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ArXDS0tl-OR"
      },
      "source": [
        "# Word2Vec 임베딩 + LSTM 레이어를 사용한 모델링\n",
        "model2 = Sequential([Embedding(vocab_size, embedding_dim, input_length =max_length),\n",
        "        # LSTM 레이어 3층\n",
        "        tf.keras.layers.LSTM(units = 64, return_sequences = True),\n",
        "        tf.keras.layers.LSTM(units = 64, return_sequences = True),\n",
        "        tf.keras.layers.LSTM(units = 64),\n",
        "        # Dense 레이어 (softmax 활성화 함수)\n",
        "        Dense(5, activation='softmax')    # 결과값이 0~4 이므로 Dense(5)\n",
        "    ])\n",
        "\n",
        "model2.compile(loss= 'categorical_crossentropy', \n",
        "              optimizer= 'adam',\n",
        "              metrics = ['accuracy']) \n",
        "\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CcXwCdsknZ7"
      },
      "source": [
        "## Bi-LSTM 모델\n",
        "* Bi-LSTM의 경우 처음에는 위의 다른 모델들과 같은 relu 활성화 함수와 adam 옵티마이저를 사용했지만 이후 다양한 활성화 함수와 옵티마이저를 사용하면서 비교해보았다.\n",
        "* 여기에 올린 코드는 최종적으로 실험했던 (가장 결과값이 좋았던) swish 활성화 함수와 RMSProp 옵티마이저를 사용한 코드이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-iFqFlVjv_5"
      },
      "source": [
        "# Word2Vec 임베딩 + 양방향LSTM 모델\n",
        "# 다른 모델들과 동일하게 케라스 텐서플로 활용\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # 임베딩 레이어\n",
        "    tf.keras.layers.Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length),\n",
        "    # Bi-LSTM 레이어\n",
        "    tf.keras.layers.Bidirectional(LSTM(10, return_sequences=False)),\n",
        "    # Dense 레이어 (swish 활성화 함수)\n",
        "    tf.keras.layers.Dense(24, activation='swish'),\n",
        "\t\t# Dense 레이어 (softmax 활성화 함수) - 5중 분류이므로 출력값 유닛을 5로 맞추어 준다.\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile model - RMSProp 옵티마이저 사용\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=RMSprop(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjt8HYylnCwR"
      },
      "source": [
        "## BERT 모델\n",
        "* BERT 모델의 경우 DACON 공유 코드를 사용하여 구축하였으나 약 9시간이 걸친 학습 결과 런타임이 끊어짐으로 인해 완료하지 못했다.\n",
        "* BERT 모델부터는 사전 학습된 모델이 아닌 이상 개인 컴퓨터로 돌리기에는 현실적으로 무리가 있음을 느꼈다.\n",
        "* BERT의 경우 구현 방식이 상이하여 위의 코드와 다르게 새롭게 작성하였다.\n",
        "* 학습을 완료하지는 못했으나 0.9498의 매우 높은 var_accuracy를 확인했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zL7Zu19mQ2H"
      },
      "source": [
        "# bert encode 함수\n",
        "def bert_encode(texts, tokenizer, max_len=max_len):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "        tokens += [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QwPDOgEndt9"
      },
      "source": [
        "trn = bert_encode(train.text.values, tokenizer, max_len=max_len)\n",
        "tst = bert_encode(test.text.values, tokenizer, max_len=max_len)\n",
        "y = train['author'].values\n",
        "print(trn[0].shape, tst[0].shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4yzcIJhngNo"
      },
      "source": [
        "# Bert 모델 구현\n",
        "def get_model(bert_layer, max_len=max_len):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    x = Dense(256, activation='relu')(clf_output)\n",
        "    out = Dense(n_class, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXO9VDZwnmuI"
      },
      "source": [
        "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTN06X1MnnGq"
      },
      "source": [
        "p_val = np.zeros((trn[0].shape[0], n_class))\n",
        "p_tst = np.zeros((tst[0].shape[0], n_class))\n",
        "\n",
        "for i, (i_trn, i_val) in enumerate(cv.split(trn[0], y), 1):\n",
        "    print(f'training model for CV #{i}')\n",
        "    es = EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=2,\n",
        "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "    \n",
        "    clf = get_model(bert_layer, max_len=max_len)\n",
        "    if i == 1:\n",
        "        print(clf.summary())\n",
        "        \n",
        "    clf.fit([x[i_trn] for x in trn], \n",
        "            to_categorical(y[i_trn]),\n",
        "            validation_data=([x[i_val] for x in trn], to_categorical(y[i_val])),\n",
        "            epochs=10,\n",
        "            batch_size=8,\n",
        "            callbacks=[es])\n",
        "    p_val[i_val, :] = clf.predict([x[i_val] for x in trn])\n",
        "    p_tst += clf.predict(tst) / n_fold\n",
        "    \n",
        "    del clf\n",
        "    clear_session()\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Gg2fftoL2W"
      },
      "source": [
        "### BERT 모델 결과값\n",
        "\n",
        "<img width=\"849\" alt=\"bert1\" src=\"https://user-images.githubusercontent.com/28593767/116551524-fd6a6780-a932-11eb-83a1-580df38a2e19.png\">\n",
        "\n",
        "<img width=\"1175\" alt=\"bert2\" src=\"https://user-images.githubusercontent.com/28593767/116551540-00655800-a933-11eb-9faf-831b3437c03f.png\">"
      ]
    }
  ]
}